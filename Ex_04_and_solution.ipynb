{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "enclosed-solution",
   "metadata": {},
   "source": [
    "# Project \n",
    "## Our Proposal\n",
    "We choose to divide key-biotopes by the types of habitats they contain. Then run a simple learning algorithm on Laser data (stored in `.laz` files) and see whether we can distinguish areas with that include particular Key-Biotopes from others.\n",
    "\n",
    "### Method:\n",
    "Laser measurements are available for square areas, 2.5 km X 2.5 km. To facilitate analysis, we will do it in steps:\n",
    "\n",
    "- **Step 1:** Divide these squares into smaller squares of equal areas (called henceforth tiles)\n",
    "- **Step 2:** Compute the percentage of area within a tile which is occupied by key-biotopes. Then, according to these percentages, label tiles for whether they contain key-bishops or not, i.e. a positive set and a negative set.\n",
    "- **Step 3:** From the laser data, compute a set of variables that characterize each tile.\n",
    "- **Step 4:** Apply a classification algorithm on the resulting dataset.\n",
    "\n",
    "### Recap\n",
    "In the last set of exercises you have went through step 1 and most step 2. You have produced shapefiles of tiles, describing their location, ratio of their area occupied by key-biotopes (coniferous forests) and referencing `.laz` files that contain the laser measurement.\n",
    "In this set of exercises you will first choose a dataset, a subset of tiles. Then you will load the corresponding `.laz` files and summerize the data in each to a handful of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ed7be",
   "metadata": {},
   "source": [
    "# Select a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7068d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25280773",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "- To Do:\n",
    "1. Load the tiles shapefiles from the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57207ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_gdf = gpd.read_file('files/tiles_shapefiles/tiles.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fb6f7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 569600 entries, 0 to 569599\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count   Dtype   \n",
      "---  ------     --------------   -----   \n",
      " 0   Tile_ID    569600 non-null  object  \n",
      " 1   square     569600 non-null  object  \n",
      " 2   Block      569600 non-null  object  \n",
      " 3   Las_Namn   569600 non-null  object  \n",
      " 4   x          569600 non-null  int64   \n",
      " 5   y          569600 non-null  int64   \n",
      " 6   bio_ratio  569600 non-null  float64 \n",
      " 7   is_KeyBio  569600 non-null  int64   \n",
      " 8   geometry   569600 non-null  geometry\n",
      "dtypes: float64(1), geometry(1), int64(3), object(4)\n",
      "memory usage: 39.1+ MB\n"
     ]
    }
   ],
   "source": [
    "tiles_gdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc41878",
   "metadata": {},
   "source": [
    "## Pick a Dataset Randomly?\n",
    "- To Do:\n",
    "1. How much of the dataset is positive (contains key-biotopes)?\n",
    "2. If we used a simple classification rule: \"all tiles belong to negative class and don't contain key-biotopes\", how much of the time we will be correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bb6e888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17% of the dataset is positive.\n",
      "The ratio of positive class to negative class is 1:594.8\n"
     ]
    }
   ],
   "source": [
    "N = len(tiles_gdf)\n",
    "ptv = len(tiles_gdf[tiles_gdf.is_KeyBio == 1])\n",
    "ntv = N - ptv\n",
    "\n",
    "print('%.2f%% of the dataset is positive.'%(ptv/ N*100))\n",
    "print('The ratio of positive class to negative class is 1:%.1f'%(ntv/ptv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b76046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning all tiles as negative is correct 99.83% of the time\n"
     ]
    }
   ],
   "source": [
    "print('Assigning all tiles as negative is correct %.2f%% of the time'%(ntv/N*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600a24d4",
   "metadata": {},
   "source": [
    "Most of the machine learning algorithms used for classification are designed with the assumption of an equal number of instances for each class. A dataset which has many more instances of one class is called **imbalanced**.\n",
    "While an imbalanced dataset can be the result of miscollecting the data, in some problems it is expected. In our problem, it is natural to expect that majority of the area is not covered by key-biotopes.\n",
    "\n",
    "One way to deal with this imbalance is to *resample* the data. We can choose a subset of the data that contain almost equal numbers of instances in each class.\n",
    "- To Do:\n",
    "1. Pick a subset of the dataset randomly, but make sure it is not imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9657cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da7dd10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptv_ind = tiles_gdf[tiles_gdf.is_KeyBio == 1].index.to_numpy()\n",
    "ntv_ind = tiles_gdf[tiles_gdf.is_KeyBio == 0].index.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf743b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = np.random.choice(ptv_ind, N_SAMPLE//2, replace= False)\n",
    "l2 = np.random.choice(ntv_ind, N_SAMPLE//2, replace= False)\n",
    "l = np.concatenate([l1, l2])\n",
    "\n",
    "sample_ind = np.random.choice(l, N_SAMPLE, replace= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a028610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_gdf = tiles_gdf.loc[sample_ind,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d869ed42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.00% of the dataset is positive.\n",
      "The ratio of positive class to negative class is 1:1.0\n"
     ]
    }
   ],
   "source": [
    "N = len(sample_gdf)\n",
    "ptv = len(sample_gdf[sample_gdf.is_KeyBio == 1])\n",
    "ntv = N - ptv\n",
    "\n",
    "print('%.2f%% of the dataset is positive.'%(ptv/ N*100))\n",
    "print('The ratio of positive class to negative class is 1:%.1f'%(ntv/ptv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd66a74",
   "metadata": {},
   "source": [
    "# Step 3:\n",
    "Now that you have a dataset of tiles, it is time to get the actual laser measurement that describes these tiles.\n",
    "\n",
    "## 3.1 Introduction to Lidar and `. Las` Files:\n",
    "**Read this section to understand the code provided** \n",
    "\n",
    "### Lidar\n",
    "Lidar (light detection and ranging) is an optical remote-sensing technique. It is a method to determine ranges (variable distance) by targeting an object with a laser and measuring the time for the reflected light to return to the receiver.\n",
    "Lidar is used primarily in airborne laser mapping to densely sample the surface of the earth, producing highly accurate x, y, z measurements.\n",
    "\n",
    "For each laser pulse, several attributes are recorded in addition to x, y and z values. This additional information includes\n",
    "- intensity: the return strength of the laser pulse,\n",
    "- return number & number of returns: an emitted laser pulse can have up to 5 returns depending on the reflecting surface,\n",
    "- point classification value: a classification defines the type of object which reflected the laser pulse. Lidar points can be classified into a number of categories including bare earth or ground, top of canopy, and water. Different classes are defined by integer codes.\n",
    "\n",
    "### LAS and LAZ Files\n",
    "Lidar produces mass point cloud datasets which are stored in `.las` files or their compressed version `.laz` files.\n",
    "\n",
    "A LAS file contains a number of fields for each point in the lidar point cloud. Those fields (or dimensions) include coordinates X, Y and Z, intensity, return_number, number_of_returns, scan_direction_flag, edge_of_flight_line, classification, etc.\n",
    "\n",
    "A LAS file also has a header. It contains metadata about the file, including information about the coordinating reference system (CRS), range of elevation, scale and offset of coordinates. \n",
    "\n",
    "See for more about [Lidar](https://pro.arcgis.com/en/pro-app/latest/help/data/las-dataset/what-is-lidar-.htm)\n",
    "\n",
    "\n",
    "### Reading LAZ files\n",
    "\n",
    "- We use module `pylas` to read these files\n",
    "\n",
    "```python\n",
    "import pylas\n",
    "\n",
    "las_data = pylas.read('path/to/file.laz') \n",
    "\n",
    "```\n",
    "\n",
    "- Read the coordinates, classification value and other attributes\n",
    "\n",
    "```python\n",
    "\n",
    "x, y ,z = las_data.X, las_data.Y, las_data.Z\n",
    "classification = las_data.classification\n",
    "```\n",
    "\n",
    "- Sometimes the coordinates need to be scaled and offset with some value stored in the header.\n",
    "\n",
    "```python\n",
    "x = las_data.X * las_data.header.x_scale + las_data.header.x_offset\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578297c",
   "metadata": {},
   "source": [
    "## 3.2 Summarize LAZ Files?\n",
    "LAZ files store millions of points, each with multiple attributes, that cover some surface area. We want to boil this huge amount of data down to a few variables that can meaningfully characterize the area in question. However, there is no clear way to do this. We don't know which variables could fulfill this purpose, how many variables should we search for? It might even be impossible to get a summary this way.\n",
    "\n",
    "A systematic solution to this problem should be an iterative process, where the set of variables will be tested in machine learning and subsequently improved.\n",
    "\n",
    "**In this project we will carry one such test: In the following code we chose to summarize each LAZ file with 30 variables: percentage of points, mean height \"Z_mean\" and standard diviation of height \"Z_std\" for classification values 1 & 2 and for each return number 1,...,5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13a3c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a laz file's name, bounds of a tile, and returns a list of 30 varibales that summarize laser\n",
    "# measurements of the tile. \n",
    "\n",
    "# path_to_dir: path to directory where laz files are stored\n",
    "\n",
    "import pylas\n",
    "\n",
    "def summary_variables(path_to_dir, file_name, x_min, y_min, x_max, y_max):\n",
    "    var = [np.nan]*30\n",
    "    \n",
    "    try:\n",
    "        laz = pylas.read(path_to_dir + file_name[:6] + '/' + file_name.replace('.laz.laz', '.laz'))\n",
    "    except:\n",
    "        return var\n",
    "    else:\n",
    "        x = laz.X * laz.header.x_scale + laz.header.x_offset\n",
    "        ind_x = (x >= x_min) & (x <= x_max )\n",
    "        y = laz.Y[ind_x] * laz.header.y_scale + laz.header.y_offset\n",
    "        ind_y = (y >= y_min) & (y <= y_max )\n",
    "        \n",
    "        z = laz.Z[ind_x][ind_y] * laz.header.z_scale + laz.header.z_offset\n",
    "        cls = laz.classification[ind_x][ind_y]\n",
    "        rn = laz.return_number[ind_x][ind_y]\n",
    "        num_pts = len(z)\n",
    "        \n",
    "        i = 0\n",
    "        for c in [1,2]:\n",
    "            ind_c = (cls==c)\n",
    "            z_c = z[ind_c]\n",
    "            \n",
    "            if len(z_c) > 0:\n",
    "                for n in [1,2,3,4,5]:\n",
    "                    ind_n = (rn[ind_c] == n)\n",
    "                    z_n = z_c[ind_n]\n",
    "                    var[i] = len(z_n)/num_pts\n",
    "                    \n",
    "                    if len(z_n) > 0:\n",
    "                        var[i+1] = z_n.mean()\n",
    "                        var[i+2] = z_n.std()\n",
    "                        i += 3\n",
    "                    else:\n",
    "                        i += 3\n",
    "                        continue\n",
    "            else:\n",
    "                i += 15\n",
    "                continue\n",
    "        return var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f08a02",
   "metadata": {},
   "source": [
    "- To Do:\n",
    "1. In a new DataFrame, use the function provided to write the `summary_variables` corresponding to your dataset.\n",
    "2. Save the resulting DataFrame into a `.csv` file for new step of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4994669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_NAME = 'D:/forest_server/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47af5cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Tile_ID', 'square', 'Block', 'Las_Namn', 'x', 'y', 'bio_ratio',\n",
       "       'is_KeyBio', 'geometry'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd74bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sample_gdf.loc[:,['Tile_ID', 'Las_Namn', 'bio_ratio', 'is_KeyBio']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2b5d3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3h 8min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(len(sample_gdf)):\n",
    "    file_name = sample_gdf.Las_Namn.iloc[i]\n",
    "    Xmin, Ymin, Xmax, Ymax = sample_gdf.geometry.iloc[i].bounds\n",
    "    \n",
    "    las_var = summary_variables(DIR_NAME, file_name, Xmin, Ymin, Xmax, Ymax)\n",
    "    data[i][4:] = las_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "497eb733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=['Tile_ID', 'Las_Namn', 'bio_ratio', 'is_KeyBio',\n",
    "                                'C1RN1_count', 'C1RN1_Zmean', 'C1RN1_Zstd', 'C1RN2_count', 'C1RN2_Zmean', 'C1RN2_Zstd',\n",
    "                                'C1RN3_count', 'C1RN3_Zmean', 'C1RN3_Zstd', 'C1RN4_count', 'C1RN4_Zmean', 'C1RN4_Zstd',\n",
    "                                'C1RN5_count', 'C1RN5_Zmean', 'C1RN5_Zstd', 'C2RN1_count', 'C2RN1_Zmean', 'C2RN1_Zstd',\n",
    "                                'C2RN2_count', 'C2RN2_Zmean', 'C2RN2_Zstd', 'C2RN3_count', 'C2RN3_Zmean', 'C2RN3_Zstd',\n",
    "                                'C2RN4_count', 'C2RN4_Zmean', 'C2RN4_Zstd', 'C2RN5_count', 'C2RN5_Zmean', 'C2RN5_Zstd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d88f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('files/learning_data/learning_data.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0582fe41",
   "metadata": {},
   "source": [
    "# Important Note:\n",
    "\n",
    "The `.laz` files are available from [lantmäteriet website](https://www.lantmateriet.se/en/maps-and-geographic-information/geodataprodukter/produktlista/laserdata-nedladdning-skog/). However, those files are very large (the total is >2TB, we used here is tens of GBs), hence it might be better to remove the part dealing with them from the exercises, and instead provide the summarized dataset as \"learning_data.csv\" file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "geo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
